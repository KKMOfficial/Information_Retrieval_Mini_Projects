{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data And Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "docs = pd.read_csv('hw1_docs.csv')\n",
    "queries = pd.read_csv('hw1_queries.csv')\n",
    "qrels = pd.read_csv('hw1_qrels.csv')\n",
    "with open('stop_words.txt','r') as f: stops = { w:1 for w in f.read().splitlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check function\n",
    "def check_results(query_index, ranking):\n",
    "    print(f\"[Query]: {queries.iloc[query_index]['query']}\")\n",
    "    RELATED_DOCS = ranking[query_index]\n",
    "    print(f\"[RELATED DOCS INDECES]: {RELATED_DOCS}\")\n",
    "    print(f\"[RELATED DOCS IDS]: {[docs.iloc[i]['doc_id'] for i in RELATED_DOCS]}\")\n",
    "    print(f\"TOP FIVE RESULT FOR 1'st QUERY\")\n",
    "    print(f\"[{docs.iloc[RELATED_DOCS[0]]['doc_id']}]: {docs.iloc[RELATED_DOCS[0]]['document']}\")\n",
    "    print(f\"[{docs.iloc[RELATED_DOCS[1]]['doc_id']}]: {docs.iloc[RELATED_DOCS[1]]['document']}\")\n",
    "    print(f\"[{docs.iloc[RELATED_DOCS[2]]['doc_id']}]: {docs.iloc[RELATED_DOCS[2]]['document']}\")\n",
    "    print(f\"[{docs.iloc[RELATED_DOCS[3]]['doc_id']}]: {docs.iloc[RELATED_DOCS[3]]['document']}\")\n",
    "    print(f\"[{docs.iloc[RELATED_DOCS[4]]['doc_id']}]: {docs.iloc[RELATED_DOCS[4]]['document']}\")\n",
    "    print(f\"[QREL]: {qrels[qrels['query_id']==(query_index+1)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Document Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing : tokenization, lower case, stop-word removal\n",
    "term_doc = {}\n",
    "doc_term = {}\n",
    "for index, row in docs.iterrows():\n",
    "    term_doc[row['doc_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['document'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in doc_term: \n",
    "            doc_term[token] += [row['doc_id']]\n",
    "        elif (((len(token)>=2) and any(not char.isdigit() for char in token)) or (len(token)>=4)) and (token not in stops): \n",
    "            doc_term[token] = [row['doc_id']]\n",
    "        else: continue\n",
    "        term_doc[row['doc_id']] += [token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency In Documents And Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most frequent terms\n",
    "# constants\n",
    "DIC_LENGTH = 10000\n",
    "NUM_RELATD = 10\n",
    "EPSILON = 1e-10\n",
    "\n",
    "doc_term = dict(sorted(doc_term.items(), key=lambda item: len(item[1])))\n",
    "# make dictionary of 1000 important words\n",
    "dictionary = {term:index for index,term in enumerate(list(doc_term.keys())[0:DIC_LENGTH])}\n",
    "# make dictionary-term tf-idf matrix for docs\n",
    "# this operation will result the DT matrix of shape(1000,750), which is 750 doc vectors in dictionary space\n",
    "TD = np.zeros((len(dictionary),len(term_doc)))\n",
    "for index,(doc,terms) in enumerate(term_doc.items()):\n",
    "    doc_in_dict = {}\n",
    "    for term in terms:\n",
    "        if term in dictionary: \n",
    "            if term in doc_in_dict:\n",
    "                doc_in_dict[term] += 1\n",
    "            else:\n",
    "                doc_in_dict[term] = 1\n",
    "    for term in doc_in_dict.keys():\n",
    "        TD[dictionary[term],index] = (doc_in_dict[term]/len(doc_in_dict))\n",
    "# make dictionary-term tf-idf matrix for queries\n",
    "# this operation will result the DT matrix of shape(1000,50), which is 50 query vectors in dictionary space\n",
    "query_term = {}\n",
    "term_query = {}\n",
    "for index, row in queries.iterrows():\n",
    "    term_query[row['query_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['query'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in dictionary:\n",
    "            if token in query_term:\n",
    "                query_term[token] += [row['query_id']]\n",
    "            else:\n",
    "                query_term[token] = [row['query_id']]\n",
    "            term_query[row['query_id']] += [token]\n",
    "\n",
    "TQ = np.zeros((len(dictionary),queries.shape[0]))\n",
    "for index,(query,terms) in enumerate(term_query.items()):\n",
    "    query_in_dict = {}\n",
    "    for term in terms:\n",
    "        if term in query_in_dict:\n",
    "            query_in_dict[term] += 1\n",
    "        else:\n",
    "            query_in_dict[term] = 1\n",
    "    for term in query_in_dict.keys():\n",
    "        TQ[dictionary[term],index] = (query_in_dict[term]/len(query_in_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jelinek-Mercer smoothing : given λ, will calculate and return smoothed TD vector\n",
    "def JMS(TD=TD, λ=0.2): return λ*(TD/(np.sum(TD,axis=0)+EPSILON)) + (1-λ)*(np.sum(TD,axis=1)/np.sum(TD)).reshape(-1,1)\n",
    "# Unigrams assumes independence of query words given document model\n",
    "def Unigram(TD=TD,TQ=TQ, λ=0.2): return np.argsort(-np.exp(np.einsum(\"ij,ik->jk\",np.log(JMS(TD=TD, λ=λ)),TQ/np.sum(TQ,axis=0))),axis=0).T[:, :NUM_RELATD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]: what is the origin of COVID-19\n",
      "[RELATED DOCS INDECES]: [637 690 668 389 425 557 435 748 609 278]\n",
      "[RELATED DOCS IDS]: ['lj8t52yl', '2y452utz', '0cq5ee1i', '0xkz36bj', '22fc1qly', '0m5mc320', '41378qru', '105q161g', '1mjaycee', '1vkz0b0o']\n",
      "TOP FIVE RESULT FOR 1'st QUERY\n",
      "[lj8t52yl]: The origin of the SARS‐CoV‐2 virus remains enigmatic. It is likely to be a continuum resulting from inevitable mutations and recombination events. These genetic changes keep developing in the present epidemic. Mutations tending to deplete the genome in its cytosine content will progressively lead to attenuation as a consequence of Muller's ratchet, but this is counteracted by recombination when different mutants co‐infect the same host, in particular, in clusters of infection. Monitoring as a function of time the genome sequences in closely related cases is critical to anticipate the future of SARS‐CoV‐2 and hence of COVID‐19.\n",
      "[2y452utz]: A novel coronavirus strain 2019-nCoV has caused a rapid global pandemic-COVID-19. Scientists have taken onto the task of characterizing this new virus and understanding how this virus has transmitted to humans. All preliminary studies have found some striking similarities between this new virus and the SARS-CoV that caused a similar kind of epidemic in 2002–2003. Through bioinformatics tools, a great deal of information has been gathered about the origin, evolution and zoonosis of this virus. We, in this review, report the symptoms, mode of transmission and available and putative treatments to tackle 2019-nCoV infections. We also comprehensively summarize all the information so far made available regarding the genome, evolution and zoonosis of this virus.\n",
      "[0cq5ee1i]: INTRODUCTION: SARS-CoV-2 was first detected in December 2019 in the Chinese city of Wuhan and has since spread across the world. At present, the virus has infected over 1.7 million people and caused over 100 000 deaths worldwide. Research is currently focused on understanding the acute infection and developing effective treatment strategies. In view of the magnitude of the epidemic, we conducted a speculative review of possible medium- and long-term neurological consequences of SARS-CoV-2 infection, with particular emphasis on neurodegenerative and neuropsychiatric diseases of neuroinflammatory origin, based on the available evidence on neurological symptoms of acute SARS-CoV-2 infection. DEVELOPMENT: We systematically reviewed the available evidence about the pathogenic mechanisms of SARS-CoV-2 infection, the immediate and lasting effects of the cytokine storm on the central nervous system, and the consequences of neuroinflammation for the central nervous system. CONCLUSIONS: SARS-CoV-2 is a neuroinvasive virus capable of triggering a cytokine storm, with persistent effects in specific populations. Although our hypothesis is highly speculative, the impact of SARS-CoV-2 infection on the onset and progression of neurodegenerative and neuropsychiatric diseases of neuroinflammatory origin should be regarded as the potential cause of a delayed pandemic that may have a major public health impact in the medium to long term. Cognitive and neuropsychological function should be closely monitored in COVID-19 survivors.\n",
      "[0xkz36bj]: INTRODUCTION: SARS-CoV-2 was first detected in December 2019 in the Chinese city of Wuhan and has since spread across the world. At present, the virus has infected over 1.7 million people and caused over 100 000 deaths worldwide. Research is currently focused on understanding the acute infection and developing effective treatment strategies. In view of the magnitude of the epidemic, we conducted a speculative review of possible medium- and long-term neurological consequences of SARS-CoV-2 infection, with particular emphasis on neurodegenerative and neuropsychiatric diseases of neuroinflammatory origin, based on the available evidence on neurological symptoms of acute SARS-CoV-2 infection. DEVELOPMENT: We systematically reviewed the available evidence about the pathogenic mechanisms of SARS-CoV-2 infection, the immediate and lasting effects of the cytokine storm on the central nervous system, and the consequences of neuroinflammation for the central nervous system. CONCLUSIONS: SARS-CoV-2 is a neuroinvasive virus capable of triggering a cytokine storm, with persistent effects in specific populations. Although our hypothesis is highly speculative, the impact of SARS-CoV-2 infection on the onset and progression of neurodegenerative and neuropsychiatric diseases of neuroinflammatory origin should be regarded as the potential cause of a delayed pandemic that may have a major public health impact in the medium to long term. Cognitive and neuropsychological function should be closely monitored in COVID-19 survivors.\n",
      "[22fc1qly]: Coronaviruses are the well-known cause of severe respiratory, enteric and systemic infections in a wide range of hosts including man, mammals, fish, and avian. The scientific interest on coronaviruses increased after the emergence of Severe Acute Respiratory Syndrome coronavirus (SARS-CoV) outbreaks in 2002-2003 followed by Middle East Respiratory Syndrome CoV (MERS-CoV). This decade's first CoV, named 2019-nCoV, emerged from Wuhan, China, and declared as 'Public Health Emergency of International Concern' on January 30th, 2020 by the World Health Organization (WHO). As on February 4, 2020, 425 deaths reported in China only and one death outside China (Philippines). In a short span of time, the virus spread has been noted in 24 countries. The zoonotic transmission (animal-to-human) is suspected as the route of disease origin. The genetic analyses predict bats as the most probable source of 2019-nCoV though further investigations needed to confirm the origin of the novel virus. The ongoing nCoV outbreak highlights the hidden wild animal reservoir of the deadly viruses and possible threat of spillover zoonoses as well. The successful virus isolation attempts have made doors open for developing better diagnostics and effective vaccines helping in combating the spread of the virus to newer areas.\n",
      "[QREL]:     query_id    doc_id\n",
      "0          1  005b2j4b\n",
      "1          1  0chuwvg6\n",
      "2          1  0t2a5500\n",
      "3          1  0y34yxlb\n",
      "4          1  105q161g\n",
      "5          1  11edrkav\n",
      "6          1  1abp6oom\n",
      "7          1  1mjaycee\n",
      "8          1  1sq2uvur\n",
      "9          1  22fc1qly\n",
      "10         1  23yi8so0\n",
      "11         1  24yavi1w\n",
      "12         1  25va2cvt\n",
      "13         1  262bcl7h\n",
      "14         1  2l4xxu3v\n"
     ]
    }
   ],
   "source": [
    "UGM = [Unigram(λ=λ/10) for λ in range(1,10,1)]\n",
    "\n",
    "check_results(0, UGM[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making a Reduced Dictionary for Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing : tokenization, lower case, stop-word removal\n",
    "term_doc = {}\n",
    "doc_term = {}\n",
    "for index, row in docs.iterrows():\n",
    "    term_doc[row['doc_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['document'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in doc_term: \n",
    "            doc_term[token] += [row['doc_id']]\n",
    "        elif (((len(token)>=2) and any(not char.isdigit() for char in token)) or (len(token)>=4)) and (token not in stops): \n",
    "            doc_term[token] = [row['doc_id']]\n",
    "        else: continue\n",
    "        term_doc[row['doc_id']] += [token]\n",
    "term_query = {}\n",
    "query_term = {}\n",
    "for index, row in queries.iterrows():\n",
    "    term_query[row['query_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['query'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in query_term: \n",
    "            query_term[token] += [row['query_id']]\n",
    "        elif (((len(token)>=2) and any(not char.isdigit() for char in token)) or (len(token)>=4)) and (token not in stops): \n",
    "            query_term[token] = [row['query_id']]\n",
    "        else: continue\n",
    "        term_query[row['query_id']] += [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most frequent terms\n",
    "# constants\n",
    "DIC_LENGTH = 700\n",
    "NUM_RELATD = 10\n",
    "EPSILON = 1e-10\n",
    "\n",
    "doc_term = dict(sorted(doc_term.items(), key=lambda item: len(item[1])))\n",
    "query_term = dict(sorted(query_term.items(), key=lambda item: len(item[1])))\n",
    "# mergy two dictinaries\n",
    "merged_dict = query_term.copy()  # Create a copy of dict1\n",
    "for key, value in doc_term.items():\n",
    "    if key in merged_dict:\n",
    "        merged_dict[key] += value\n",
    "    else:\n",
    "        merged_dict[key] = value\n",
    "# make dictionary of DIC_LENGTH important words\n",
    "dictionary = {term:index for index,term in enumerate(list(merged_dict.keys())[0:DIC_LENGTH])}\n",
    "# make dictionary-term tf-idf matrix for docs\n",
    "# this operation will result the TD matrix of shape(500,750), which is 750 doc vectors in dictionary space\n",
    "TD = np.zeros((len(dictionary),len(term_doc)))\n",
    "# this operation will also result the TTD tensor of shape(500,500,750) which models the w_i,w_i-1,D term in bigram or equivalently doc vectors in term x prev_term space!\n",
    "TTD = np.zeros((len(dictionary),len(dictionary),len(term_doc)))\n",
    "for index,(doc,terms) in enumerate(term_doc.items()):\n",
    "    doc_in_dict = {}\n",
    "    doc_in_bi_dict = {}\n",
    "    prev_term = None\n",
    "    for term in terms:\n",
    "        if term in dictionary: \n",
    "            # uni dicitonary\n",
    "            if term in doc_in_dict:\n",
    "                doc_in_dict[term] += 1\n",
    "            else:\n",
    "                doc_in_dict[term] = 1\n",
    "            # bi dictionary\n",
    "            if prev_term != None:\n",
    "                if (term,prev_term) in doc_in_bi_dict:\n",
    "                    doc_in_bi_dict[(term,prev_term)] += 1\n",
    "                else:\n",
    "                    doc_in_bi_dict[(term,prev_term)] = 1\n",
    "            prev_term = term\n",
    "        else:\n",
    "            prev_term = None\n",
    "    # assemble uni dictionary\n",
    "    for term in doc_in_dict.keys():\n",
    "        TD[dictionary[term],index] = (doc_in_dict[term]/len(doc_in_dict))\n",
    "    # assemble bi dictionary\n",
    "    for (term,prev_term) in doc_in_bi_dict.keys():\n",
    "        TTD[dictionary[term],dictionary[prev_term],index] = (doc_in_bi_dict[(term,prev_term)]/doc_in_dict[prev_term])\n",
    "# make dictionary-term tf-idf matrix for queries\n",
    "# this operation will result the DT matrix of shape(1000,50), which is 50 query vectors in dictionary space\n",
    "query_term = {}\n",
    "term_query = {}\n",
    "for index, row in queries.iterrows():\n",
    "    term_query[row['query_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['query'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in dictionary:\n",
    "            if token in query_term:\n",
    "                query_term[token] += [row['query_id']]\n",
    "            else:\n",
    "                query_term[token] = [row['query_id']]\n",
    "            term_query[row['query_id']] += [token]\n",
    "# this operation will result the TQ matrix of shape(500,50), which is 50 query vectors in dictionary space\n",
    "TQ = np.zeros((len(dictionary),queries.shape[0]))\n",
    "# this operation will also result the TTQ tensor of shape(500,500,50) which models the w_i,w_i-1,Q term in bigram or equivalently query vectors in term x prev_term space!\n",
    "TTQ = np.zeros((len(dictionary), len(dictionary), queries.shape[0]))\n",
    "for index,(query,terms) in enumerate(term_query.items()):\n",
    "    query_in_dict = {}\n",
    "    query_in_bi_dict = {}\n",
    "    prev_term = None\n",
    "    for term in terms:\n",
    "        # uni dictionary\n",
    "        if term in query_in_dict:\n",
    "            query_in_dict[term] += 1\n",
    "        else:\n",
    "            query_in_dict[term] = 1\n",
    "        # bi dictionary\n",
    "        if prev_term != None:\n",
    "            if (term,prev_term) in query_in_bi_dict:\n",
    "                query_in_bi_dict[(term,prev_term)] += 1\n",
    "            else:\n",
    "                query_in_bi_dict[(term,prev_term)] = 1\n",
    "        prev_term = term\n",
    "    # assemble uni dictionary\n",
    "    for term in query_in_dict.keys():\n",
    "        TQ[dictionary[term],index] = (query_in_dict[term]/len(query_in_dict))\n",
    "    # assemble bi dictionary\n",
    "    for (term,prev_term) in query_in_bi_dict.keys():\n",
    "        TTQ[dictionary[term],dictionary[prev_term],index] = (query_in_bi_dict[(term,prev_term)]/query_in_dict[prev_term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 50)\n",
      "(700, 700, 50)\n",
      "(700, 750)\n",
      "(700, 700, 750)\n"
     ]
    }
   ],
   "source": [
    "print(TQ.shape)\n",
    "print(TTQ.shape)\n",
    "print(TD.shape)\n",
    "print(TTD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jelinek-Mercer smoothing : given λ_1, λ_2, will calculate and return smoothed TTD vector\n",
    "def JMS(TTD=TTD, TD=TD, λ_1=0.2, λ_2=0.2): return λ_1*TTD + (λ_2*(TD/(np.sum(TD,axis=0)+EPSILON)) + (1-λ_1-λ_2)*(np.sum(TD,axis=1)/(np.sum(TD)+EPSILON)).reshape(-1,1)).reshape((DIC_LENGTH,1,-1))\n",
    "# Bigrams assumes independence of query words given document model and one previous term if exists\n",
    "def Bigram(TTD=TTD, TTQ=TTQ, TD=TD, TQ=TQ, λ_1=0.2, λ_2=0.2): return np.argsort(-np.einsum(\"ijd,ijq->dq\",np.log(JMS(TTD=TTD, TD=TD, λ_1=λ_1, λ_2=λ_2)+1e-12),TTQ), axis=0).T[:, :NUM_RELATD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BGM = [Bigram(λ_1=λ_1, λ_2=λ_2) for λ_1,λ_2 in [(0.6,0.1),(0.6,0.2),(0.6,0.3),(0.7,0.1),(0.7,0.2),(0.8,0.1),(0.9,0.05)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]: what is the origin of COVID-19\n",
      "[RELATED DOCS INDECES]: [721 525 654 520 158 614 209 494 739 415]\n",
      "[RELATED DOCS IDS]: ['1emlkii0', '0u00nhf2', '45bwzuqn', '0xqhm8a0', '04zbbyii', '2lebavgm', '16crg3k8', '1xj2sg4y', '42t0zriz', '4dnzjeyp']\n",
      "TOP FIVE RESULT FOR 1'st QUERY\n",
      "[1emlkii0]: COVID-19 is a bluff\n",
      "[0u00nhf2]: OBJECTIVES: To investigate the different CT characteristics which may distinguish influenza from 2019 coronavirus disease (COVID-19). METHODS: A total of 13 confirmed patients with COVID-19 were enrolled from January 16, 2020, to February 25, 2020. Furthermore, 92 CT scans of confirmed patients with influenza pneumonia, including 76 with influenza A and 16 with influenza B, scanned between January 1, 2019, to February 25, 2020, were retrospectively reviewed. Pulmonary lesion distributions, number, attenuation, lobe predomination, margin, contour, ground-glass opacity involvement pattern, bronchial wall thickening, air bronchogram, tree-in-bud sign, interlobular septal thickening, intralobular septal thickening, and pleural effusion were evaluated in COVID-19 and influenza pneumonia cohorts. RESULTS: Peripheral and non-specific distributions in COVID-19 showed a markedly higher frequency compared with the influenza group (p < 0.05). Most lesions in COVID-19 showed balanced lobe localization, while in influenza pneumonia they were predominantly located in the inferior lobe (p < 0.05). COVID-19 presented a clear lesion margin and a shrinking contour compared with influenza pneumonia (p < 0.05). COVID-19 had a patchy or combination of GGO and consolidation opacities, while a cluster-like pattern and bronchial wall thickening were more frequently seen in influenza pneumonia (p < 0.05). The lesion number and attenuation, air bronchogram, tree-in-bud sign, interlobular septal thickening, and intralobular septal thickening were not significantly different between the two groups (all p > 0.05). CONCLUSIONS: Though viral pneumonias generally show similar imaging features, there are some characteristic CT findings which may help differentiating COVID-19 from influenza pneumonia. KEY POINTS: • CT can play an early warning role in the diagnosis of COVID-19 in the case of no epidemic exposure. • CT could be used for the differential diagnosis of influenza and COVID-19 with satisfactory accuracy. • COVID-19 had a patchy or combination of GGO and consolidation opacities with peripheral distribution and balanced lobe predomination.\n",
      "[45bwzuqn]: OBJECTIVES: To investigate the clinical and chest CT characteristics of COVID-19 pneumonia and explore the radiological differences between COVID-19 and influenza. MATERIALS AND METHODS: A total of 122 patients (61 men and 61 women, 48 ± 15 years) confirmed with COVID-19 and 48 patients (23 men and 25 women, 47 ± 19 years) confirmed with influenza were enrolled in the study. Thin-section CT was performed. The clinical data and the chest CT findings were recorded. RESULTS: The most common symptoms of COVID-19 were fever (74%) and cough (63%), and 102 patients (83%) had Wuhan contact. Pneumonia in 50 patients with COVID-19 (45%) distributed in the peripheral regions of the lung, while it showed mixed distribution in 26 patients (74%) with influenza (p = 0.022). The most common CT features of the COVID-19 group were pure ground-glass opacities (GGO, 36%), GGO with consolidation (51%), rounded opacities (35%), linear opacities (64%), bronchiolar wall thickening (49%), and interlobular septal thickening (66%). Compared with the influenza group, the COVID-19 group was more likely to have rounded opacities (35% vs. 17%, p = 0.048) and interlobular septal thickening (66% vs. 43%, p = 0.014), but less likely to have nodules (28% vs. 71%, p < 0.001), tree-in-bud sign (9% vs. 40%, p < 0.001), and pleural effusion (6% vs. 31%, p < 0.001). CONCLUSIONS: There are significant differences in the CT manifestations of patients with COVID-19 and influenza. Presence of rounded opacities and interlobular septal thickening, with the absence of nodules and tree-in-bud sign, and with the typical peripheral distribution, may help us differentiate COVID-19 from influenza. KEY POINTS: • Typical CT features of COVID-19 include pure ground-glass opacities (GGO), GGO with consolidation, rounded opacities, bronchiolar wall thickening, interlobular septal thickening, and a peripheral distribution. • Presence of rounded opacities and interlobular septal thickening, with the absence of nodules and tree-in-bud sign, and with the typical peripheral distribution, may help us differentiate COVID-19 from influenza.\n",
      "[0xqhm8a0]: Recently, the cholera outbreak in Haiti demonstrated just how unprepared the country is to rapidly isolate an outbreak of this magnitude, and its vulnerability to the COVID-19 pandemic. This communication briefly examines the health system in Haiti and its vulnerability toward the COVID-19 outbreak.\n",
      "[04zbbyii]: Olfactory and taste dysfunctions have emerged as symptoms of COVID-19. Among individuals with COVID-19 enrolled in a household study, loss of taste and/or smell was the fourth most commonly reported symptom (26/42; 62%), and among household contacts, it had the highest positive predictive value (83%; 95% CI: 55-95%) for COVID-19. These findings support consideration of loss of taste and/or smell in possible case identification and testing prioritization for COVID-19.\n",
      "[QREL]:     query_id    doc_id\n",
      "0          1  005b2j4b\n",
      "1          1  0chuwvg6\n",
      "2          1  0t2a5500\n",
      "3          1  0y34yxlb\n",
      "4          1  105q161g\n",
      "5          1  11edrkav\n",
      "6          1  1abp6oom\n",
      "7          1  1mjaycee\n",
      "8          1  1sq2uvur\n",
      "9          1  22fc1qly\n",
      "10         1  23yi8so0\n",
      "11         1  24yavi1w\n",
      "12         1  25va2cvt\n",
      "13         1  262bcl7h\n",
      "14         1  2l4xxu3v\n"
     ]
    }
   ],
   "source": [
    "check_results(0, BGM[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most frequent terms\n",
    "# constants\n",
    "DIC_LENGTH = 10000\n",
    "NUM_RELATD = 10\n",
    "EPSILON = 1e-10\n",
    "\n",
    "doc_term = dict(sorted(doc_term.items(), key=lambda item: len(item[1])))\n",
    "query_term = dict(sorted(query_term.items(), key=lambda item: len(item[1])))\n",
    "# mergy two dictinaries\n",
    "merged_dict = query_term.copy()  # Create a copy of dict1\n",
    "for key, value in doc_term.items():\n",
    "    if key in merged_dict:\n",
    "        merged_dict[key] += value\n",
    "    else:\n",
    "        merged_dict[key] = value\n",
    "# make dictionary of DIC_LENGTH important words\n",
    "dictionary = {term:index for index,term in enumerate(list(merged_dict.keys()))}\n",
    "# make dictionary-term tf-idf matrix for docs\n",
    "# this operation will result the DT matrix of shape(1000,750), which is 750 doc vectors in dictionary space\n",
    "TD = np.zeros((len(dictionary),len(term_doc)))\n",
    "for index,(doc,terms) in enumerate(term_doc.items()):\n",
    "    doc_in_dict = {}\n",
    "    for term in terms:\n",
    "        if term in dictionary: \n",
    "            if term in doc_in_dict:\n",
    "                doc_in_dict[term] += 1\n",
    "            else:\n",
    "                doc_in_dict[term] = 1\n",
    "    for term in doc_in_dict.keys():\n",
    "        TD[dictionary[term],index] = (doc_in_dict[term]/len(doc_in_dict)) * np.log10(len(term_doc)/len(doc_term[term])) \n",
    "# make dictionary-term tf-idf matrix for queries\n",
    "# this operation will result the TQ matrix of shape(1000,50), which is 50 query vectors in dictionary space\n",
    "query_term = {}\n",
    "term_query = {}\n",
    "for index, row in queries.iterrows():\n",
    "    term_query[row['query_id']] = []\n",
    "    tokens_list = re.split(r'\\s+', re.sub(r'[^\\w\\s]',' ',row['query'].lower()))\n",
    "    for token in tokens_list:\n",
    "        if token in dictionary:\n",
    "            if token in query_term:\n",
    "                query_term[token] += [row['query_id']]\n",
    "            else:\n",
    "                query_term[token] = [row['query_id']]\n",
    "            term_query[row['query_id']] += [token]\n",
    "\n",
    "TQ = np.zeros((len(dictionary),queries.shape[0]))\n",
    "for index,(query,terms) in enumerate(term_query.items()):\n",
    "    query_in_dict = {}\n",
    "    for term in terms:\n",
    "        if term in query_in_dict:\n",
    "            query_in_dict[term] += 1\n",
    "        else:\n",
    "            query_in_dict[term] = 1\n",
    "    for term in query_in_dict.keys():\n",
    "        TQ[dictionary[term],index] = (query_in_dict[term]/len(query_in_dict)) * np.log10(len(term_doc)/len(doc_term[term])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]: what is the origin of COVID-19\n",
      "[RELATED DOCS INDECES]: [396  20 690 748 425 736 490 122 217 701]\n",
      "[RELATED DOCS IDS]: ['24yavi1w', '0t2a5500', '2y452utz', '105q161g', '22fc1qly', '0chuwvg6', '1sq2uvur', '1fxrmuzl', 'vk8s1f23', 'ig0rnbqb']\n",
      "TOP FIVE RESULT FOR 1'st QUERY\n",
      "[24yavi1w]: Mutation and adaptation have driven the co-evolution of coronaviruses (CoVs) and their hosts, including human beings, for thousands of years. Before 2003, two human CoVs (HCoVs) were known to cause mild illness, such as common cold. The outbreaks of severe acute respiratory syndrome (SARS) and the Middle East respiratory syndrome (MERS) have flipped the coin to reveal how devastating and life-threatening an HCoV infection could be. The emergence of SARS-CoV-2 in central China at the end of 2019 has thrusted CoVs into the spotlight again and surprised us with its high transmissibility but reduced pathogenicity compared to its sister SARS-CoV. HCoV infection is a zoonosis and understanding the zoonotic origins of HCoVs would serve us well. Most HCoVs originated from bats where they are non-pathogenic. The intermediate reservoir hosts of some HCoVs are also known. Identifying the animal hosts has direct implications in the prevention of human diseases. Investigating CoV-host interactions in animals might also derive important insight on CoV pathogenesis in humans. In this review, we present an overview of the existing knowledge about the seven HCoVs, with a focus on the history of their discovery as well as their zoonotic origins and interspecies transmission. Importantly, we compare and contrast the different HCoVs from a perspective of virus evolution and genome recombination. The current CoV disease 2019 (COVID-19) epidemic is discussed in this context. In addition, the requirements for successful host switches and the implications of virus evolution on disease severity are also highlighted.\n",
      "[0t2a5500]: The Chinese rufous horseshoe bat (Rhinolophus sinicus) has been suggested to carry the direct ancestor of severe acute respiratory syndrome (SARS) coronavirus (SCoV), and the diversity of SARS-like CoVs (SLCoV) within this Rhinolophus species is therefore worth investigating. Here, we demonstrate the remarkable diversity of SLCoVs in R. sinicus and identify a strain with the same pattern of phylogenetic incongruence (i.e. an indication of recombination) as reported previously in another SLCoV strain. Moreover, this strain possesses a distinctive 579 nt deletion in the nsp3 region that was also found in a human SCoV from the late-phase epidemic. Phylogenetic analysis of the Orf1 region suggested that the human SCoVs are phylogenetically closer to SLCoVs in R. sinicus than to SLCoVs in other Rhinolophus species. These findings reveal a closer evolutionary linkage between SCoV in humans and SLCoVs in R. sinicus, defining the scope of surveillance to search for the direct ancestor of human SCoVs.\n",
      "[2y452utz]: A novel coronavirus strain 2019-nCoV has caused a rapid global pandemic-COVID-19. Scientists have taken onto the task of characterizing this new virus and understanding how this virus has transmitted to humans. All preliminary studies have found some striking similarities between this new virus and the SARS-CoV that caused a similar kind of epidemic in 2002–2003. Through bioinformatics tools, a great deal of information has been gathered about the origin, evolution and zoonosis of this virus. We, in this review, report the symptoms, mode of transmission and available and putative treatments to tackle 2019-nCoV infections. We also comprehensively summarize all the information so far made available regarding the genome, evolution and zoonosis of this virus.\n",
      "[105q161g]: A number of virological, epidemiological and ethnographic arguments suggest that COVID-19 has a zoonotic origin. The pangolin, a species threatened with extinction due to poaching for both culinary purposes and traditional Chinese pharmacopoeia, is now suspected of being the “missing link” in the transmission to humans of a virus that probably originated in a species of bat. Our predation of wild fauna and the reduction in their habitats have thus ended up creating new interfaces that favour the transmission of pathogens (mainly viruses) to humans. Domesticated animals and wild fauna thus constitute a reservoir for almost 80% of emerging human diseases (SARS-CoV, MERS-CoV, Ebola). These diseases are all zoonotic in origin. As if out of a Chinese fairy tale, the bat and the pangolin have taught us a lesson: within an increasingly interdependent world, environmental crises will become ever more intertwined with health crises. Questions relating to public health will no longer be confined to the secrecy of the physician’s consulting room or the sanitised environment of the hospital. They are now being played out in the arena of international trade, ports and airports and distribution networks. Simply put, all human activity creates new interfaces that facilitate the transmission of pathogens from an animal reservoir to humans. This pluri-disciplinary article highlights that environmental changes, such as the reduction in habitats for wild fauna and the intemperate trade in fauna, are the biggest causes of the emergence of new diseases. Against this background, it reviews the different measures taken to control, eradicate and prevent the emergence of animal diseases in a globalised world.\n",
      "[22fc1qly]: Coronaviruses are the well-known cause of severe respiratory, enteric and systemic infections in a wide range of hosts including man, mammals, fish, and avian. The scientific interest on coronaviruses increased after the emergence of Severe Acute Respiratory Syndrome coronavirus (SARS-CoV) outbreaks in 2002-2003 followed by Middle East Respiratory Syndrome CoV (MERS-CoV). This decade's first CoV, named 2019-nCoV, emerged from Wuhan, China, and declared as 'Public Health Emergency of International Concern' on January 30th, 2020 by the World Health Organization (WHO). As on February 4, 2020, 425 deaths reported in China only and one death outside China (Philippines). In a short span of time, the virus spread has been noted in 24 countries. The zoonotic transmission (animal-to-human) is suspected as the route of disease origin. The genetic analyses predict bats as the most probable source of 2019-nCoV though further investigations needed to confirm the origin of the novel virus. The ongoing nCoV outbreak highlights the hidden wild animal reservoir of the deadly viruses and possible threat of spillover zoonoses as well. The successful virus isolation attempts have made doors open for developing better diagnostics and effective vaccines helping in combating the spread of the virus to newer areas.\n",
      "[QREL]:     query_id    doc_id\n",
      "0          1  005b2j4b\n",
      "1          1  0chuwvg6\n",
      "2          1  0t2a5500\n",
      "3          1  0y34yxlb\n",
      "4          1  105q161g\n",
      "5          1  11edrkav\n",
      "6          1  1abp6oom\n",
      "7          1  1mjaycee\n",
      "8          1  1sq2uvur\n",
      "9          1  22fc1qly\n",
      "10         1  23yi8so0\n",
      "11         1  24yavi1w\n",
      "12         1  25va2cvt\n",
      "13         1  262bcl7h\n",
      "14         1  2l4xxu3v\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(list(term_doc.values())+list(term_query.values()), min_count=1, sg=1, vector_size=200, epochs=20)\n",
    "# Arithmatic Mean vectors\n",
    "doc_vec = np.array([np.array([model.wv[term] for term in terms]).mean(axis=0) for (doc,terms) in term_doc.items()])\n",
    "query_vec = np.array([np.array([model.wv[term] for term in terms]).mean(axis=0) for (query,terms) in term_query.items()])\n",
    "\n",
    "# Weighted Mean vectors\n",
    "weighted_doc_vec = np.array([np.array([model.wv[term]*TD[dictionary[term],index] for term in terms]).mean(axis=0) for index,(doc,terms) in enumerate(term_doc.items())])\n",
    "weighted_query_vec = np.array([np.array([model.wv[term]*TQ[dictionary[term],index] for term in terms]).mean(axis=0) for index,(query,terms) in enumerate(term_query.items())])\n",
    "\n",
    "AW2V = np.argsort(-np.einsum(\"qi,di->qd\",query_vec,doc_vec)/(np.linalg.norm(query_vec, axis=1).reshape((-1,1))*np.linalg.norm(doc_vec, axis=1).reshape((1,-1))), axis=1)[:, :NUM_RELATD]\n",
    "WW2V = np.argsort(-np.einsum(\"qi,di->qd\",weighted_query_vec,weighted_doc_vec)/(np.linalg.norm(weighted_query_vec, axis=1).reshape((-1,1))*np.linalg.norm(weighted_doc_vec, axis=1).reshape((1,-1))), axis=1)[:, :NUM_RELATD]\n",
    "\n",
    "check_results(0, WW2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]: what is the origin of COVID-19\n",
      "[RELATED DOCS INDECES]: [276 721 425 690 609 435 420 131 351 683]\n",
      "[RELATED DOCS IDS]: ['2lxs9laj', '1emlkii0', '22fc1qly', '2y452utz', '1mjaycee', '41378qru', '12sbikmx', '03pd9jtn', '0uvzy48c', '4r0t3q7j']\n",
      "TOP FIVE RESULT FOR 1'st QUERY\n",
      "[2lxs9laj]: Coronavirus disease 2019 (COVID-19), which causes serious respiratory illness such as pneumonia and lung failure, was first reported in Wuhan, the capital of Hubei, China. The etiological agent of COVID-19 has been confirmed as a novel coronavirus, now known as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which is most likely originated from zoonotic coronaviruses, like SARS-CoV, which emerged in 2002. Within a few months of the first report, SARS-CoV-2 had spread across China and worldwide, reaching a pandemic level. As COVID-19 has triggered enormous human casualties and serious economic loss posing global threat, an understanding of the ongoing situation and the development of strategies to contain the virus's spread are urgently needed. Currently, various diagnostic kits to test for COVID-19 are available and several repurposing therapeutics for COVID-19 have shown to be clinically effective. In addition, global institutions and companies have begun to develop vaccines for the prevention of COVID-19. Here, we review the current status of epidemiology, diagnosis, treatment, and vaccine development for COVID-19.\n",
      "[1emlkii0]: COVID-19 is a bluff\n",
      "[22fc1qly]: Coronaviruses are the well-known cause of severe respiratory, enteric and systemic infections in a wide range of hosts including man, mammals, fish, and avian. The scientific interest on coronaviruses increased after the emergence of Severe Acute Respiratory Syndrome coronavirus (SARS-CoV) outbreaks in 2002-2003 followed by Middle East Respiratory Syndrome CoV (MERS-CoV). This decade's first CoV, named 2019-nCoV, emerged from Wuhan, China, and declared as 'Public Health Emergency of International Concern' on January 30th, 2020 by the World Health Organization (WHO). As on February 4, 2020, 425 deaths reported in China only and one death outside China (Philippines). In a short span of time, the virus spread has been noted in 24 countries. The zoonotic transmission (animal-to-human) is suspected as the route of disease origin. The genetic analyses predict bats as the most probable source of 2019-nCoV though further investigations needed to confirm the origin of the novel virus. The ongoing nCoV outbreak highlights the hidden wild animal reservoir of the deadly viruses and possible threat of spillover zoonoses as well. The successful virus isolation attempts have made doors open for developing better diagnostics and effective vaccines helping in combating the spread of the virus to newer areas.\n",
      "[2y452utz]: A novel coronavirus strain 2019-nCoV has caused a rapid global pandemic-COVID-19. Scientists have taken onto the task of characterizing this new virus and understanding how this virus has transmitted to humans. All preliminary studies have found some striking similarities between this new virus and the SARS-CoV that caused a similar kind of epidemic in 2002–2003. Through bioinformatics tools, a great deal of information has been gathered about the origin, evolution and zoonosis of this virus. We, in this review, report the symptoms, mode of transmission and available and putative treatments to tackle 2019-nCoV infections. We also comprehensively summarize all the information so far made available regarding the genome, evolution and zoonosis of this virus.\n",
      "[1mjaycee]: The coronavirus disease 19 (COVID-19) is a highly transmittable and pathogenic viral infection caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which emerged in Wuhan, China and spread around the world. Genomic analysis revealed that SARS-CoV-2 is phylogenetically related to severe acute respiratory syndrome-like (SARS-like) bat viruses, therefore bats could be the possible primary reservoir. The intermediate source of origin and transfer to humans is not known, however, the rapid human to human transfer has been confirmed widely. There is no clinically approved antiviral drug or vaccine available to be used against COVID-19. However, few broad-spectrum antiviral drugs have been evaluated against COVID-19 in clinical trials, resulted in clinical recovery. In the current review, we summarize and comparatively analyze the emergence and pathogenicity of COVID-19 infection and previous human coronaviruses severe acute respiratory syndrome coronavirus (SARS-CoV) and middle east respiratory syndrome coronavirus (MERS-CoV). We also discuss the approaches for developing effective vaccines and therapeutic combinations to cope with this viral outbreak.\n",
      "[QREL]:     query_id    doc_id\n",
      "0          1  005b2j4b\n",
      "1          1  0chuwvg6\n",
      "2          1  0t2a5500\n",
      "3          1  0y34yxlb\n",
      "4          1  105q161g\n",
      "5          1  11edrkav\n",
      "6          1  1abp6oom\n",
      "7          1  1mjaycee\n",
      "8          1  1sq2uvur\n",
      "9          1  22fc1qly\n",
      "10         1  23yi8so0\n",
      "11         1  24yavi1w\n",
      "12         1  25va2cvt\n",
      "13         1  262bcl7h\n",
      "14         1  2l4xxu3v\n"
     ]
    }
   ],
   "source": [
    "check_results(0, AW2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "doc_ids = [torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])[:, :512] for sentence in list(docs['document'])]\n",
    "query_ids = [torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])[:, :512] for sentence in list(queries['query'])]\n",
    "with torch.no_grad():\n",
    "    # results = []\n",
    "    for doc_id in tqdm(doc_ids):\n",
    "        try: results += [model(doc_id).last_hidden_state.mean(dim=1).squeeze()]\n",
    "        except: print(f\"[error] : {doc_id}\")\n",
    "    doc_vec = torch.stack(results)\n",
    "    results = []\n",
    "    for query_id in tqdm(query_ids):\n",
    "        try: results += [model(query_id).last_hidden_state.mean(dim=1).squeeze()]\n",
    "        except: print(f\"[error] : {query_id}\")\n",
    "    query_vec = torch.stack(results)\n",
    "import pickle\n",
    "with open (\"bert_vectors.pth\", \"wb\") as f:\n",
    "    pickle.dump((doc_vec, query_vec), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"bert_vectors.pth\", \"rb\") as f:\n",
    "    (doc_vec, query_vec) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query]: how does the coronavirus respond to changes in the weather\n",
      "[RELATED DOCS INDECES]: [602 104 261 637 717 285 701 559 139 476]\n",
      "[RELATED DOCS IDS]: ['03s9spbi', '431ksdno', '15slu3kk', 'lj8t52yl', '0pujch9v', '04awj06g', 'ig0rnbqb', '6vln3erl', '0vlzwksu', '8r6u3e3i']\n",
      "TOP FIVE RESULT FOR 1'st QUERY\n",
      "[03s9spbi]: (1) Background: The virulence of coronavirus diseases due to viruses like SARS-CoV or MERS-CoV decreases in humid and hot weather. The putative temperature dependence of infectivity by the new coronavirus SARS-CoV-2 or covid-19 has a high predictive medical interest. (2) Methods: External temperature and new covid-19 cases in 21 countries and in the French administrative regions were collected from public data. Associations between epidemiological parameters of the new case dynamics and temperature were examined using an ARIMA model. (3) Results: We show that, in the first stages of the epidemic, the velocity of contagion decreases with country- or region-wise temperature. (4) Conclusions: Results indicate that high temperatures diminish initial contagion rates, but seasonal temperature effects at later stages of the epidemy remain questionable. Confinement policies and other eviction rules should account for climatological heterogeneities, in order to adapt the public health decisions to possible geographic or seasonal gradients.\n",
      "[431ksdno]: The coronavirus and the influenza virus have similarities and differences. In order to comprehensively compare them, their genome sequencing data were examined by principal component analysis. Variations in coronavirus were smaller than those in a subclass of the influenza virus. In addition, differences among coronaviruses in a variety of hosts were small. These characteristics may have facilitated the infection of different hosts. Although many of the coronaviruses were more conservative, those repeatedly found among humans showed annual changes. If SARS-CoV-2 changes its genome like the Influenza H type, it will repeatedly spread every few years. In addition, the coronavirus family has many other candidates for subsequent pandemics. One Sentence Summary The genome data of coronavirus were compared to influenza virus, to investigate its spreading mechanism and future status. Coronavirus would repeatedly spread every few years. In addition, the coronavirus family has many other candidates for subsequent pandemics.\n",
      "[15slu3kk]: SARS CoV-2 (COVID-19) Coronavirus cases are confirmed throughout the world and millions of people are being put into quarantine. A better understanding of the effective parameters in infection spreading can bring about a logical measurement toward COVID-19. The effect of climatic factors on spreading of COVID-19 can play an important role in the new Coronavirus outbreak. In this study, the main parameters, including the number of infected people with COVID-19, population density, intra-provincial movement, and infection days to end of the study period, average temperature, average precipitation, humidity, wind speed, and average solar radiation investigated to understand how can these parameters effects on COVID-19 spreading in Iran? The Partial correlation coefficient (PCC) and Sobol'-Jansen methods are used for analyzing the effect and correlation of variables with the COVID-19 spreading rate. The result of sensitivity analysis shows that the population density, intra-provincial movement have a direct relationship with the infection outbreak. Conversely, areas with low values of wind speed, humidity, and solar radiation exposure to a high rate of infection that support the virus's survival. The provinces such as Tehran, Mazandaran, Alborz, Gilan, and Qom are more susceptible to infection because of high population density, intra-provincial movements and high humidity rate in comparison with Southern provinces.\n",
      "[lj8t52yl]: The origin of the SARS‐CoV‐2 virus remains enigmatic. It is likely to be a continuum resulting from inevitable mutations and recombination events. These genetic changes keep developing in the present epidemic. Mutations tending to deplete the genome in its cytosine content will progressively lead to attenuation as a consequence of Muller's ratchet, but this is counteracted by recombination when different mutants co‐infect the same host, in particular, in clusters of infection. Monitoring as a function of time the genome sequences in closely related cases is critical to anticipate the future of SARS‐CoV‐2 and hence of COVID‐19.\n",
      "[0pujch9v]: We don't know if changing seasons will help stem the outbreak, says Michael Le Page\n",
      "[QREL]:     query_id    doc_id\n",
      "15         2  01goni72\n",
      "16         2  03s9spbi\n",
      "17         2  04awj06g\n",
      "18         2  04rbtmmi\n",
      "19         2  0oma7hdu\n",
      "20         2  0pujch9v\n",
      "21         2  0vlzwksu\n",
      "22         2  0y62eqdx\n",
      "23         2  11pcdnlw\n",
      "24         2  147yc66p\n",
      "25         2  15slu3kk\n",
      "26         2  1bxt21za\n",
      "27         2  1dq91x2r\n",
      "28         2  1k3d3o2q\n",
      "29         2  1llox90t\n"
     ]
    }
   ],
   "source": [
    "DIC_LENGTH = 10000\n",
    "NUM_RELATD = 10\n",
    "EPSILON = 1e-10\n",
    "BERTR = torch.argsort(-torch.einsum(\"qi,di->qd\",query_vec,doc_vec)/(torch.norm(query_vec, dim=1).reshape((-1,1))*torch.norm(doc_vec, dim=1).reshape((1,-1))), axis=1)[:, :NUM_RELATD].numpy()\n",
    "check_results(1, BERTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unigram_λ=0.1   MRR]: 0.72\n",
      "[Unigram_λ=0.2   MRR]: 0.74\n",
      "[Unigram_λ=0.3   MRR]: 0.74\n",
      "[Unigram_λ=0.4   MRR]: 0.74\n",
      "[Unigram_λ=0.5   MRR]: 0.74\n",
      "[Unigram_λ=0.6   MRR]: 0.73\n",
      "[Unigram_λ=0.7   MRR]: 0.73\n",
      "[Unigram_λ=0.8   MRR]: 0.73\n",
      "[Unigram_λ=0.9   MRR]: 0.75\n",
      "[Bigram, λ_1=0.6, λ_2=0.1   MRR]: 0.65\n",
      "[Bigram, λ_1=0.6, λ_2=0.2   MRR]: 0.69\n",
      "[Bigram, λ_1=0.6, λ_2=0.3   MRR]: 0.71\n",
      "[Bigram, λ_1=0.7, λ_2=0.1   MRR]: 0.66\n",
      "[Bigram, λ_1=0.7, λ_2=0.2   MRR]: 0.70\n",
      "[Bigram, λ_1=0.8, λ_2=0.1   MRR]: 0.66\n",
      "[Bigram, λ_1=0.9, λ_2=0.1   MRR]: 0.66\n",
      "[Arithmatic W2V   MRR]: 0.69\n",
      "[Weighted W2V   MRR]: 0.83\n",
      "[BERT   MRR]: 0.42\n"
     ]
    }
   ],
   "source": [
    "def RR(rank, ground_truth):\n",
    "    for i,doc_id in enumerate(rank):\n",
    "        if doc_id in ground_truth : return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "BGλ = [(0.6,0.1),(0.6,0.2),(0.6,0.3),(0.7,0.1),(0.7,0.2),(0.8,0.1),(0.9,0.05)]\n",
    "\n",
    "for ind,Rank in enumerate(UGM):\n",
    "    print(f\"[Unigram_λ={0.1*(ind+1):.1f}   MRR]: {np.array([RR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "for ind,Rank in enumerate(BGM):\n",
    "    print(f\"[Bigram, λ_1={BGλ[ind][0]:.1f}, λ_2={BGλ[ind][1]:.1f}   MRR]: {np.array([RR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "print(f\"[Arithmatic W2V   MRR]: {np.array([RR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(AW2V)]).mean():.2f}\")\n",
    "print(f\"[Weighted W2V   MRR]: {np.array([RR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(WW2V)]).mean():.2f}\")\n",
    "print(f\"[BERT   MRR]: {np.array([RR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(BERTR)]).mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unigram_λ=0.1   MAP]: 0.48\n",
      "[Unigram_λ=0.2   MAP]: 0.49\n",
      "[Unigram_λ=0.3   MAP]: 0.48\n",
      "[Unigram_λ=0.4   MAP]: 0.46\n",
      "[Unigram_λ=0.5   MAP]: 0.45\n",
      "[Unigram_λ=0.6   MAP]: 0.45\n",
      "[Unigram_λ=0.7   MAP]: 0.45\n",
      "[Unigram_λ=0.8   MAP]: 0.45\n",
      "[Unigram_λ=0.9   MAP]: 0.45\n",
      "[Bigram, λ_1=0.6, λ_2=0.1   MAP]: 0.39\n",
      "[Bigram, λ_1=0.6, λ_2=0.2   MAP]: 0.40\n",
      "[Bigram, λ_1=0.6, λ_2=0.3   MAP]: 0.40\n",
      "[Bigram, λ_1=0.7, λ_2=0.1   MAP]: 0.39\n",
      "[Bigram, λ_1=0.7, λ_2=0.2   MAP]: 0.40\n",
      "[Bigram, λ_1=0.8, λ_2=0.1   MAP]: 0.39\n",
      "[Bigram, λ_1=0.9, λ_2=0.1   MAP]: 0.38\n",
      "[Arithmatic W2V   MAP]: 0.42\n",
      "[Weighted W2V   MAP]: 0.48\n",
      "[BERT   MAP]: 0.18\n"
     ]
    }
   ],
   "source": [
    "def PR(rank, ground_truth):\n",
    "    score, correct = [], 1\n",
    "    for i,doc_id in enumerate(rank):\n",
    "        score += [(correct if doc_id in ground_truth else 0)/(i+1)]\n",
    "        correct += 1\n",
    "    return np.array(score).mean()\n",
    "\n",
    "for ind,Rank in enumerate(UGM):\n",
    "    print(f\"[Unigram_λ={0.1*(ind+1):.1f}   MAP]: {np.array([PR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "for ind,Rank in enumerate(BGM):\n",
    "    print(f\"[Bigram, λ_1={BGλ[ind][0]:.1f}, λ_2={BGλ[ind][1]:.1f}   MAP]: {np.array([PR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "print(f\"[Arithmatic W2V   MAP]: {np.array([PR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(AW2V)]).mean():.2f}\")\n",
    "print(f\"[Weighted W2V   MAP]: {np.array([PR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(WW2V)]).mean():.2f}\")\n",
    "print(f\"[BERT   MAP]: {np.array([PR(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list()) for i,rank in enumerate(BERTR)]).mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P@K Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unigram_λ=0.1   P@5]: 0.55\n",
      "[Unigram_λ=0.2   P@5]: 0.56\n",
      "[Unigram_λ=0.3   P@5]: 0.56\n",
      "[Unigram_λ=0.4   P@5]: 0.55\n",
      "[Unigram_λ=0.5   P@5]: 0.55\n",
      "[Unigram_λ=0.6   P@5]: 0.53\n",
      "[Unigram_λ=0.7   P@5]: 0.53\n",
      "[Unigram_λ=0.8   P@5]: 0.53\n",
      "[Unigram_λ=0.9   P@5]: 0.53\n",
      "[Bigram, λ_1=0.6, λ_2=0.1   P@5]: 0.44\n",
      "[Bigram, λ_1=0.6, λ_2=0.2   P@5]: 0.48\n",
      "[Bigram, λ_1=0.6, λ_2=0.3   P@5]: 0.51\n",
      "[Bigram, λ_1=0.7, λ_2=0.1   P@5]: 0.45\n",
      "[Bigram, λ_1=0.7, λ_2=0.2   P@5]: 0.49\n",
      "[Bigram, λ_1=0.8, λ_2=0.1   P@5]: 0.45\n",
      "[Bigram, λ_1=0.9, λ_2=0.1   P@5]: 0.44\n",
      "[Arithmatic W2V   P@5]: 0.49\n",
      "[Weighted W2V   P@5]: 0.62\n",
      "[BERT   P@5]: 0.18\n"
     ]
    }
   ],
   "source": [
    "def PK(rank, ground_truth, k):\n",
    "    correct = 0\n",
    "    for _,doc_id in enumerate(rank[:k]):\n",
    "        if doc_id in ground_truth: correct += 1\n",
    "    return correct/k\n",
    "\n",
    "for ind,Rank in enumerate(UGM):\n",
    "    print(f\"[Unigram_λ={0.1*(ind+1):.1f}   P@5]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=5) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "for ind,Rank in enumerate(BGM):\n",
    "    print(f\"[Bigram, λ_1={BGλ[ind][0]:.1f}, λ_2={BGλ[ind][1]:.1f}   P@5]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=5) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "print(f\"[Arithmatic W2V   P@5]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=5) for i,rank in enumerate(AW2V)]).mean():.2f}\")\n",
    "print(f\"[Weighted W2V   P@5]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=5) for i,rank in enumerate(WW2V)]).mean():.2f}\")\n",
    "print(f\"[BERT   P@5]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=5) for i,rank in enumerate(BERTR)]).mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Unigram_λ=0.1   P@10]: 0.48\n",
      "[Unigram_λ=0.2   P@10]: 0.49\n",
      "[Unigram_λ=0.3   P@10]: 0.48\n",
      "[Unigram_λ=0.4   P@10]: 0.46\n",
      "[Unigram_λ=0.5   P@10]: 0.45\n",
      "[Unigram_λ=0.6   P@10]: 0.45\n",
      "[Unigram_λ=0.7   P@10]: 0.45\n",
      "[Unigram_λ=0.8   P@10]: 0.45\n",
      "[Unigram_λ=0.9   P@10]: 0.45\n",
      "[Bigram, λ_1=0.6, λ_2=0.1   P@10]: 0.39\n",
      "[Bigram, λ_1=0.6, λ_2=0.2   P@10]: 0.40\n",
      "[Bigram, λ_1=0.6, λ_2=0.3   P@10]: 0.40\n",
      "[Bigram, λ_1=0.7, λ_2=0.1   P@10]: 0.39\n",
      "[Bigram, λ_1=0.7, λ_2=0.2   P@10]: 0.40\n",
      "[Bigram, λ_1=0.8, λ_2=0.1   P@10]: 0.39\n",
      "[Bigram, λ_1=0.9, λ_2=0.1   P@10]: 0.38\n",
      "[Arithmatic W2V   P@10]: 0.42\n",
      "[Weighted W2V   P@10]: 0.48\n",
      "[BERT   P@10]: 0.18\n"
     ]
    }
   ],
   "source": [
    "for ind,Rank in enumerate(UGM):\n",
    "    print(f\"[Unigram_λ={0.1*(ind+1):.1f}   P@10]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=10) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "for ind,Rank in enumerate(BGM):\n",
    "    print(f\"[Bigram, λ_1={BGλ[ind][0]:.1f}, λ_2={BGλ[ind][1]:.1f}   P@10]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=10) for i,rank in enumerate(Rank)]).mean():.2f}\")\n",
    "print(f\"[Arithmatic W2V   P@10]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=10) for i,rank in enumerate(AW2V)]).mean():.2f}\")\n",
    "print(f\"[Weighted W2V   P@10]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=10) for i,rank in enumerate(WW2V)]).mean():.2f}\")\n",
    "print(f\"[BERT   P@10]: {np.array([PK(docs.iloc[rank]['doc_id'].to_list(), qrels[qrels['query_id']==(i+1)]['doc_id'].to_list(), k=10) for i,rank in enumerate(BERTR)]).mean():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
